Etapa onde se realizam os processos de tratamento dos dados e inserção no banco de dados.
Arquivo: ./datasets_tratados/tratamento_dataset.ipynb
    - 1 tratamento do dataset original. preenchimento de nulos com 0.
    - Remoção do rodapé com textos desnecessarios.
    - Adicionar  
    - retirada de linhas vazias.

Arquivo : .datasets_rotacionados/ rotacionar_csv.ipynb
    - Aqui o codigo é feito pra rotacionar a tabela deixando com as colunas (Produto/Unidade,Nível de Comercialização,U.F.,data,valor)
    para inserir no postgres foi necessário retirar a virgula e o ponto da cassa do milhar. deixar nesse formato 10000.00

Arquivo: ./merge_csv/merge_colunas.ipynb 
    - aruivo feito para criar as dimensões. crias os csv para fazer os produtos e os tipos de comercio.

Nesse tempo foi criada uma pasta funções pois alguns codigos estavam se replicando muito.
as funções criadas são:
    - atualizar_macrogrupo.py
    - conectar_banco.py (concetar com banco de dados)
    - fechar_conexao (fechar conexão com banco)

No arquivo ./_4_Etapa_ETL/ insert_public, foram inseridos os dados nas dimensões.
    - datawarehouse.dim_produtos - dados de merge_colunas.ipynb - quem gerou o merge_colunas_produtos.Todos os produtos.
    - datawarehouse.dim_nivel_comercializacao - tipos de Comercialização
    - datawarehouse.dim_data - 
    - datawarehouse.dim_macrogrupo
        Foi notado que seria interessante verificar os macrogrupos, frutas, cereais, etc..Assim foi criado uma
        nova coluna chamada macrogrupo. Adicionado a fk na tabela dim_produtos e feita a dimensão macrogrupos.
    - datawarehouse.fato_produtos - feita por sql (queries no arquivo ./_8_sql/scricpts_sql.sql)

Aqui com as dimensões e a tabela_fato feita fiz o download do csv pois verifiquei que pdeoria tratar os dados auqi com python 
precisar ficar pegando do banco( boa pratica). com o codigo ./_5_Dataframes_tratados/conexao.ipynb

